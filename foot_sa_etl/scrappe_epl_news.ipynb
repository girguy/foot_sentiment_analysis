{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import pytz  # To manage time zones\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union, Optional\n",
    "import polars as pl\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPL_TEAM = { \n",
    "    'AFC Bournemouth' : 'afc-bournemouth',\n",
    "    'Arsenal' : 'arsenal',\n",
    "    'Aston Villa' : 'aston-villa',\n",
    "    'Brentford' : 'brentford',\n",
    "    'Brighton & Hove Albion' : 'brighton-and-hove-albion',\n",
    "    'Chelsea' : 'chelsea',\n",
    "    'Crystal Palace' : 'crystal-palace',\n",
    "    'Everton' : 'everton',\n",
    "    'Fulham' : 'fulham',\n",
    "    'Ipswich Town' : 'ipswich-town',\n",
    "    'Leicester City' : 'leicester-city',\n",
    "    'Liverpool' : 'liverpool',\n",
    "    'Manchester City' : 'manchester-city',\n",
    "    'Manchester United' : 'manchester-united',\n",
    "    'Newcastle United' : 'newcastle-united',\n",
    "    'Nottingham Forest' : 'nottingham-forest',\n",
    "    'Southampton' : 'southampton',\n",
    "    'Tottenham Hotspur' : 'tottenham-hotspur',\n",
    "    'West Ham United' : 'west-ham-united',\n",
    "    'Wolverhampton Wanderers' : 'wolverhampton-wanderers'\n",
    "}  \n",
    "\n",
    "PAGES_NUMBER = [1, 2]\n",
    "\n",
    "BASE_URL = \"https://www.bbc.com/sport/football/teams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blob_client_with_connection_string(connection_string: str) -> BlobServiceClient:\n",
    "    \"\"\"\n",
    "    Creates a BlobServiceClient using a connection string, handling URL-encoded characters.\n",
    "    \n",
    "    :param connection_string: The Azure Storage connection string\n",
    "    :return: BlobServiceClient object\n",
    "    \"\"\"\n",
    "    connection_string = re.sub(r'%2B', '+', connection_string)\n",
    "    return BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "\n",
    "def write_blob_to_container(df: pl.DataFrame, container_name: str, path_to_blob: str, blob_service_client: BlobServiceClient) -> None:\n",
    "    \"\"\"\n",
    "    Writes a Polars DataFrame as a Parquet file to an Azure Blob Storage container.\n",
    "    \n",
    "    :param df: Polars DataFrame to write\n",
    "    :param container_name: Name of the Azure Blob Storage container\n",
    "    :param path_to_blob: Path to the blob in the container\n",
    "    :param blob_service_client: BlobServiceClient object for Azure Blob Storage\n",
    "    \"\"\"\n",
    "    parquet_buffer = from_polars_to_parquet(df)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=path_to_blob)\n",
    "    try:\n",
    "        blob_client.upload_blob(parquet_buffer.getvalue(), blob_type=\"BlockBlob\", overwrite=True)\n",
    "        print(f\"Successfully uploaded blob to {container_name}/{path_to_blob}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading blob to {container_name}/{path_to_blob}: {e}\")\n",
    "\n",
    "\n",
    "def read_blob_from_container(container_name: str, path_to_blob: str, blob_service_client: BlobServiceClient) -> Union[pl.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Reads a Parquet file from an Azure Blob Storage container and returns it as a Polars DataFrame.\n",
    "    \n",
    "    :param container_name: Name of the Azure Blob Storage container\n",
    "    :param path_to_blob: Path to the blob in the container\n",
    "    :param blob_service_client: BlobServiceClient object for Azure Blob Storage\n",
    "    :return: Polars DataFrame read from the blob, or None if the operation fails\n",
    "    \"\"\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=path_to_blob)\n",
    "    try:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        blob_data = download_stream.readall()\n",
    "        df = pl.read_parquet(BytesIO(blob_data))\n",
    "        print(f\"Successfully read blob from {container_name}/{path_to_blob}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading blob from {container_name}/{path_to_blob}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def from_polars_to_parquet(df: pl.DataFrame) -> BytesIO:\n",
    "    \"\"\"\n",
    "    Converts a Polars DataFrame to a Parquet file in memory.\n",
    "    \n",
    "    :param df: Polars DataFrame to be converted to Parquet\n",
    "    :return: BytesIO buffer containing the Parquet file\n",
    "    \"\"\"\n",
    "    parquet_buffer = BytesIO()\n",
    "    df.write_parquet(parquet_buffer, use_pyarrow=True)\n",
    "    parquet_buffer.seek(0)  # Reset buffer position to the beginning\n",
    "    return parquet_buffer\n",
    "\n",
    "\n",
    "def merge_dataframes_on_hashedId(df1: pl.DataFrame, df2: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges two Polars DataFrames based on the \"_hashedId\" column.\n",
    "    If an \"_hashedId\" from df1 exists in df2, it will not be added.\n",
    "    \n",
    "    :param df1: First Polars DataFrame\n",
    "    :param df2: Second Polars DataFrame\n",
    "    :return: Merged Polars DataFrame with no duplicate \"_hashedId\" records from df1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform an anti-join to find records in df1 that do not have a match in df2 based on \"_hashedId\"\n",
    "        df_filtered = df1.join(df2, on=\"_hashedId\", how=\"anti\")\n",
    "        \n",
    "        # Log the result of the anti-join operation\n",
    "        if df_filtered.is_empty():\n",
    "            print(\"No new records to merge.\")\n",
    "        else:\n",
    "            print(f\"{df_filtered.shape[0]} new records found. Merging them.\")\n",
    "\n",
    "        # Concatenate the filtered rows from df1 with df2\n",
    "        df_merged = pl.concat([df2, df_filtered], how=\"vertical\")\n",
    "        return df_merged\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while merging dataframes: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_current_datetime(timezone: str = 'UTC') -> str:\n",
    "    \"\"\"\n",
    "    Returns the current date and time in the specified timezone.\n",
    "    \n",
    "    :param timezone: Timezone name, default is 'UTC'\n",
    "    :return: Current date and time as a formatted string\n",
    "    \"\"\"\n",
    "    current_datetime = datetime.now(pytz.timezone(timezone))\n",
    "    return current_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def get_teams_url(epl_teams, number_of_pages) -> List[List[Union[str, int, str]]]:\n",
    "    \"\"\"\n",
    "    Generates a list of URLs for the given teams and the specified number of pages.\n",
    "    \n",
    "    :param epl_teams: A dictionary of team names and their corresponding values\n",
    "    :param number_of_pages: The number of pages to scrape for each team\n",
    "    :return: A list of lists containing team name, page number, and URL\n",
    "    \"\"\"\n",
    "    # Dictionary to hold the URLs\n",
    "    team_urls = []\n",
    "\n",
    "    for team_name, value in epl_teams.items():\n",
    "        for page_number in range(1, number_of_pages + 1):\n",
    "            # Create a list of URLs for each team\n",
    "            url = f\"{BASE_URL}/{value}/?page={page_number}\"\n",
    "            team_urls.append([team_name, page_number, url])\n",
    "    \n",
    "    return team_urls\n",
    "\n",
    "\n",
    "def generate_hash(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a SHA-256 hash from the given content string.\n",
    "    \n",
    "    :param content: String to hash\n",
    "    :return: SHA-256 hash of the input content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_blob_name(timestamp: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a blob name in the format 'epl_news_YYYY_MM_DD' based on the given timestamp string.\n",
    "    \n",
    "    :param timestamp: A string representing the date and time (e.g., '2024-10-03 10:46:15')\n",
    "    :return: A formatted string in the format 'epl_news_YYYY_MM_DD'\n",
    "    \"\"\"\n",
    "    # Extract the date part from the timestamp\n",
    "    date_part = timestamp.split(' ')[0]\n",
    "    \n",
    "    # Replace the hyphens with underscores\n",
    "    formatted_date = date_part.replace('-', '_')\n",
    "    \n",
    "    # Return the final formatted blob name\n",
    "    return f\"epl_news_{formatted_date}\"\n",
    "\n",
    "\n",
    "def create_dataframe(input_df: List[List[Union[str, int, str]]], datetime_now: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a Polars DataFrame with the specified schema, adds the current datetime to each row,\n",
    "    and generates a hash of the HTML content.\n",
    "    \n",
    "    :param input_df: Input data as a list of lists\n",
    "    :param datetime_now: Current datetime as a string to add to the \"_extractedDate\" column\n",
    "    :return: A Polars DataFrame with the specified schema and new columns\n",
    "    \"\"\"\n",
    "    # Define the schema explicitly\n",
    "    schema = pl.Schema({\n",
    "        \"teamName\": pl.String(),\n",
    "         \"page\": pl.Int8,\n",
    "         \"html\": pl.String()\n",
    "         })\n",
    "\n",
    "    df = pl.from_records(\n",
    "        input_df,\n",
    "        schema = schema,\n",
    "        orient = 'row'\n",
    "        )\n",
    "    \n",
    "    # Create the \"_extractedDate\" column using the provided datetime\n",
    "    date_list = [datetime_now for i in range(len(input_df))]\n",
    "    date_series = pl.Series(\"_extractedDate\", date_list)\n",
    "    datetime_series = date_series.str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "    df = df.with_columns(\n",
    "        pl.Series(\"_extractedDate\", datetime_series)\n",
    "        )\n",
    "\n",
    "    # Add a new \"_hashedId\" column by applying the generate_hash function to the \"html\" column\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"html\").map_elements(lambda x: generate_hash(x), return_dtype=pl.Utf8).alias(\"_hashedId\")\n",
    "        )\n",
    "    \n",
    "    # Rearrange the columns\n",
    "    return df.select([\"_hashedId\", \"_extractedDate\", \"teamName\", \"page\", \"html\"])\n",
    "\n",
    "\n",
    "async def get_page(team_name: str, page_number: int, url: str, session: aiohttp.ClientSession) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a webpage asynchronously with retries in case of failure.\n",
    "    \n",
    "    :param team_name: Name of the team\n",
    "    :param page_number: Page number to fetch\n",
    "    :param url: The URL to request\n",
    "    :param session: An aiohttp ClientSession object for making requests\n",
    "    :return: A list containing the team name, page number, and either the HTML content or an error message\n",
    "    \"\"\"\n",
    "    retries = 3\n",
    "    backoff_factor = 2\n",
    "    delay = 1\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                if 200 <= response.status < 300:\n",
    "                    print(f\"Success: {team_name} Page {page_number}\")\n",
    "                    return [team_name, page_number, await response.text()]\n",
    "                elif response.status == 429: # too many requests code\n",
    "                        print(f\"Error {response.status} for {url}, retrying for id {id}...\")\n",
    "                        raise aiohttp.ClientError(f\"HTTP error 429 for {url}\")\n",
    "                else:\n",
    "                    print(f\"Failed with status {response.status} for {team_name}, page {page_number}\")\n",
    "                    return [team_name, page_number, f\"HTTP error {response.status}\"]\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            print(f\"Error {e}, retrying in {delay} seconds for {team_name}, page {page_number} (Attempt {attempt + 1}/{retries})\")\n",
    "            await asyncio.sleep(delay)\n",
    "            delay *= backoff_factor  # Increase the delay exponentially\n",
    "\n",
    "        # After max retries, raise the exception\n",
    "        if attempt == retries - 1:\n",
    "            print(f\"Failed after {retries} attempts for {team_name}, page {page_number}\")\n",
    "            return [team_name, page_number, f\"Failed after {retries} attempts\"]\n",
    "\n",
    "\n",
    "async def get_all_pages(teams_details: List[Tuple[str, int, str]], session: aiohttp.ClientSession) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates asynchronous tasks to fetch pages for all teams and collects the results.\n",
    "    \n",
    "    :param teams_details: A list of tuples where each tuple contains (team_name, page_number, url)\n",
    "    :param session: An aiohttp ClientSession object for making requests\n",
    "    :return: A list of lists containing the team name, page number, and page content or error messages\n",
    "    \"\"\"\n",
    "    tasks = [asyncio.create_task(get_page(team_name, page_number, url, session))\n",
    "             for team_name, page_number, url in teams_details]\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "async def scrapper(urls: List[Tuple[str, int, str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Main asynchronous entry point that sets up the aiohttp session and fetches pages for all URLs.\n",
    "    \n",
    "    :param urls: A list of tuples where each tuple contains (team_name, page_number, url)\n",
    "    :return: A list of lists containing the team name, page number, and page content or error messages\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        data = await get_all_pages(urls, session)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current datetime\n",
    "datetime_now = get_current_datetime()\n",
    "\n",
    "# Get the list of team URLs to scrape\n",
    "team_urls = get_teams_url(EPL_TEAM, 2)\n",
    "\n",
    "# Scrape the data from the URLs asynchronously\n",
    "results = await scrapper(team_urls)\n",
    "\n",
    "# Create a new Polars DataFrame from the scraped results\n",
    "df_new = create_dataframe(results, datetime_now)\n",
    "\n",
    "# Load environment variables\n",
    "connection_string = os.environ.get(\"CONN_STRING_AZURE_STORAGE\")\n",
    "if connection_string is None:\n",
    "    raise EnvironmentError(\"Azure storage connection string not found in environment variables.\")\n",
    "\n",
    "# Create a blob client for Azure Blob Storage\n",
    "blob_service_client = create_blob_client_with_connection_string(connection_string)\n",
    "\n",
    "# Define the container and path for the blob storage\n",
    "container_name = \"bronze\"\n",
    "folder_name = \"epl_news\"\n",
    "blob_name = create_blob_name(datetime_now)\n",
    "path = f\"{folder_name}/{blob_name}.parquet\"\n",
    "\n",
    "# Read the existing blob data from Azure Blob Storage, if available\n",
    "df_actual: Optional[pl.DataFrame] = read_blob_from_container(container_name, path, blob_service_client)\n",
    "\n",
    "if df_actual is None:\n",
    "    # If no existing data, write the new DataFrame directly to the blob\n",
    "    print(\"No existing data found, writing new data to blob...\")\n",
    "    write_blob_to_container(df_new, container_name, path, blob_service_client)\n",
    "else:\n",
    "    # If existing data is found, merge it with the new data\n",
    "    print(\"Existing data found, merging with new data...\")\n",
    "    df_merged = merge_dataframes_on_hashedId(df_actual, df_new)\n",
    "\n",
    "    # Write the merged DataFrame back to the blob\n",
    "    write_blob_to_container(df_merged, container_name, path, blob_service_client)\n",
    "        \n",
    "print(\"Operation completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foot-sentiment-analysis-Ku8eHlTk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
